{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9w9bNo1H4MRxxQ4zW+Xjv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bryan-Az/RL-SARSA-Gym/blob/main/reinforcement_sarsa_gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning using SARSA in OpenAI Gym / Gymnasium"
      ],
      "metadata": {
        "id": "bkTEMEEMYTPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Installs"
      ],
      "metadata": {
        "id": "jAwlWopLZRBY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hmIHBJvdXoyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "0b57901e-5335-4a9a-d56c-1b298e743f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[atari]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.5)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: farama-notifications, gymnasium, ale-py, shimmy\n",
            "Successfully installed ale-py-0.8.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-0.2.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (4.66.5)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2024.8.30)\n",
            "Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446661 sha256=6d2ddc978113487ce7435794daf145332e18b9afa6f0f831eeaecb92c029ab22\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.26.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py) (6.4.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[atari] # for the gym library\n",
        "!pip install gymnasium[accept-rom-license] #to add atari envs\n",
        "!pip install ale-py # for atari envs\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Creating the Environment"
      ],
      "metadata": {
        "id": "YyAA-9lFYZcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"MountainCar-v0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nx1awc4Yr-A",
        "outputId": "d669493c-63b6-4c93-a9e4-d7779ef352aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_name, render_mode=\"rgb_array\")"
      ],
      "metadata": {
        "id": "XQ2jSuTTY2Jx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Creating the Agent with SARSA Learning"
      ],
      "metadata": {
        "id": "1JmXPnKjYgY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the hyperparameters specific to the training within the environment and learning task. These are used within the Agent / Learner class and are similar to the training of a neural network."
      ],
      "metadata": {
        "id": "jhaVc28CsVIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MAX_NUM_EPISODES = 1000\n",
        "MAX_NUM_EPISODES = 25000 #100001\n",
        "STEPS_PER_EPISODE = 200 #  This is specific to MountainCar. May change with env\n",
        "EPSILON_MIN = 0.005\n",
        "max_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE\n",
        "EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps\n",
        "ALPHA = 0.1  # Learning rate\n",
        "GAMMA = 0.9 #1.0  # Discount factor\n",
        "NUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim"
      ],
      "metadata": {
        "id": "_LpfYrw1sTcb"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SARSA_Learner(object):\n",
        "    def __init__(self, env):\n",
        "        self.obs_shape = env.observation_space.shape\n",
        "        self.obs_high = env.observation_space.high\n",
        "        self.obs_low = env.observation_space.low\n",
        "        # printing all the obs_ for debugging\n",
        "        #print(self.obs_high, 'obs high')\n",
        "        #print(self.obs_low, ' obs low')\n",
        "        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim\n",
        "        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins\n",
        "        self.action_shape = env.action_space.n\n",
        "        # Create a multi-dimensional array (aka. Table) to represent the\n",
        "        # Q-values\n",
        "        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,\n",
        "                           self.action_shape))  # (51 x 51 x 3)\n",
        "        self.alpha = ALPHA  # Learning rate\n",
        "        self.gamma = GAMMA  # Discount factor\n",
        "        self.epsilon = 0.9\n",
        "        ### Unique to SARSA Method ###\n",
        "        rewards = []\n",
        "    ''' def discretize(self, obs):\n",
        "        # Convert the observation to a tuple of integers using list comprehension\n",
        "        # and unpack it using the * operator when indexing into self.Q\n",
        "        return tuple([int(np.round((obs[i] - self.obs_low[i]) / self.bin_width[i])) for i in range(len(obs))])\n",
        "\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        discretized_obs = self.discretize(obs)\n",
        "        # Epsilon-Greedy action selection\n",
        "        if self.epsilon > EPSILON_MIN:\n",
        "            self.epsilon -= EPSILON_DECAY\n",
        "        if np.random.random() > self.epsilon:\n",
        "            return np.argmax(self.Q[discretized_obs])  # unpack the tuple\n",
        "        else:  # Choose a random action\n",
        "            return np.random.choice([a for a in range(self.action_shape)])\n",
        "    '''\n",
        "    def discretize(self, obs):\n",
        "      #clipped_obs = np.clip(obs[0], self.obs_low, self.obs_high)\n",
        "      #return tuple(((clipped_obs - self.obs_low) / self.bin_width).astype(int))\n",
        "      #discretized_env = (self.obs_high - self.obs_low) / self.obs_bins\n",
        "      #discretized_pos = int((clipped_obs[0] - self.obs_low[0]) / discretized_env[0])\n",
        "      #discretized_vel = int((clipped_obs[1] - self.obs_low[1]) / discretized_env[1])\n",
        "      #return discretized_pos, discretized_vel\n",
        "      discrete_obs = (obs[0] - self.obs_low) / self.bin_width\n",
        "      return tuple(np.clip(discrete_obs.astype(int), 0, self.obs_bins - 1))\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        discretized_obs = self.discretize(obs)\n",
        "        # Epsilon-Greedy action selection\n",
        "        if self.epsilon > EPSILON_MIN:\n",
        "            self.epsilon -= EPSILON_DECAY\n",
        "        if np.random.random() > self.epsilon:\n",
        "            return np.argmax(self.Q[discretized_obs])\n",
        "        else:  # Choose a random action\n",
        "            return np.random.choice([a for a in range(self.action_shape)])\n",
        "\n",
        "    def learn(self, obs, action, reward, next_obs, next_action):\n",
        "        '''\n",
        "        This is the SARSA learning method that uses the get_action function\n",
        "        to retrieve the next state-action\n",
        "        as input to updating the Q learning matrix, versus the\n",
        "        max Q value of the next state as in Q Learning.\n",
        "        '''\n",
        "        discretized_obs = self.discretize(obs)\n",
        "        discretized_next_obs = self.discretize(next_obs)\n",
        "        # change self.Q[discretized_next_obs][next_action] to np.max(self.Q[discretized_next_obs])\n",
        "        # if want to use Q learning\n",
        "        td_target = reward + self.gamma * self.Q[discretized_next_obs][next_action]\n",
        "        td_error = td_target - self.Q[discretized_obs][action]\n",
        "        self.Q[discretized_obs][action] += self.alpha * td_error\n",
        "\n",
        "def train(agent, env):\n",
        "    best_reward = -float('inf')\n",
        "    for episode in range(MAX_NUM_EPISODES):\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        action = agent.get_action(obs)\n",
        "        ### printing the obs for debugging\n",
        "        #print(obs, ' obs')\n",
        "        #print(type(obs), ' obs type')\n",
        "        #print(obs[0][0], ' obs[0]')\n",
        "        #print(obs[0][1], ' obs[1]')\n",
        "        #print(obs[1], ' obs[1]')\n",
        "        #print(type(obs[0]), ' obs[0] type')\n",
        "        #print(type(obs[1]), ' obs[1] type')\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "            # retrieving the next action necessary for the SARSA learning method\n",
        "            next_action = agent.get_action(next_obs)\n",
        "            done = terminated or truncated\n",
        "            agent.learn(obs, action, reward, next_obs, next_action)\n",
        "            obs = next_obs\n",
        "            action = next_action\n",
        "            total_reward += reward\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "        print(\"Episode#:{} reward:{} best_reward:{} eps:{}\".format(episode,\n",
        "                                     total_reward, best_reward, agent.epsilon))\n",
        "    # Return the trained policy\n",
        "    return np.argmax(agent.Q, axis=2)\n",
        "\n",
        "\n",
        "def test(agent, env, policy):\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    total_reward = 0.0\n",
        "    while not done:\n",
        "        action = policy[agent.discretize(obs)]\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "    return total_reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFqOwRyEXzJh",
        "outputId": "9f1ae41b-def8-40d8-f9f7-2549783e34eb"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Instantiating & Training the Learner / Agent\n",
        "In this stage, the agent is still learning its' preference / policy. In the testing phase, the outcome of its' preferences will be monitored and visualized."
      ],
      "metadata": {
        "id": "MG1e5K21qBN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = SARSA_Learner(env)\n",
        "learned_policy = train(agent, env)"
      ],
      "metadata": {
        "id": "tpNNH9seYQ_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Evaluating the Training by Recording the Agent within the Environment"
      ],
      "metadata": {
        "id": "3B6umK60qY3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses the Gym Monitor wrapper to evalaute the agent and record video\n",
        "# only one video will be saved\n",
        "\n",
        "# video of the final episode with the episode trigger\n",
        "env = gym.wrappers.RecordVideo(\n",
        "    env, \"./gym_monitor_output\", episode_trigger=lambda x: x == 0)\n",
        "\n",
        "test(agent, env, learned_policy)\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksZkMDJwqKN1",
        "outputId": "6f197d7f-c2e1-4589-dd59-050fecba77d5"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/gym_monitor_output folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/gym_monitor_output/rl-video-episode-26000.mp4.\n",
            "Moviepy - Writing video /content/gym_monitor_output/rl-video-episode-26000.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/gym_monitor_output/rl-video-episode-26000.mp4\n",
            "Moviepy - Building video /content/gym_monitor_output/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/gym_monitor_output/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/gym_monitor_output/rl-video-episode-0.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cqh9euOR-ccV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}